{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VQA Generation and Finetuning using ABO Dataset\n",
        "We have used Amazon's dataset to generate a Visual Question Answering via prompting an LLM that accept an image as input. We have experimented with Gemini API as well as on device ones using llava models. Based on the compute available and the results generated, we decided to move with the Gemini API to generate the dataset while handling exceptions during API calls. We then evaluated a pretrained BLIP and fine-tuned it on this dataset using Low Rank Adaptation method (LoRA). More details and results can be found in the following [report](https://drive.google.com/file/d/1WTs4sVgsXIqaiwg7BRb3cV2hOJgegmXe/view?usp=sharing)\n",
        "."
      ],
      "metadata": {
        "id": "XrBu3Fzp0HLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset generation"
      ],
      "metadata": {
        "id": "BWQv0x4YNlB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX3dOuTSJJN8"
      },
      "outputs": [],
      "source": [
        "!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
        "!tar -xf abo-images-small.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyf5sEoMLlbV"
      },
      "outputs": [],
      "source": [
        "# get abo-listings.tar\n",
        "!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-listings.tar\n",
        "!tar -xf abo-listings.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0JcXM7VKq5y"
      },
      "source": [
        "### Unzipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7voII6cfJhAk"
      },
      "outputs": [],
      "source": [
        "# extract the images/metadata/images.csv.gz file\n",
        "!gzip -d images/metadata/images.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEda1HrUlwv9"
      },
      "outputs": [],
      "source": [
        "# extract listings/metadata/*.json.gz\n",
        "!gzip -d listings/metadata/*.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnOHE3-EKW7z"
      },
      "outputs": [],
      "source": [
        "# display images.csv\n",
        "!head images/metadata/images.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyKoe6g_mKgf"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPeQffuDmM7X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the file content\n",
        "\n",
        "with open('/content/listings/metadata/listings_0.json', 'r') as f:\n",
        "    file_content = f.read()\n",
        "\n",
        "# Attempt to decode JSON objects iteratively\n",
        "data = []\n",
        "decoder = json.JSONDecoder()\n",
        "while file_content:\n",
        "    try:\n",
        "        obj, index = decoder.raw_decode(file_content)\n",
        "        data.append(obj)\n",
        "        file_content = file_content[index:].lstrip()  # Remove processed data and leading whitespace\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Handle potential errors, e.g., log them or break the loop\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    break  # Or handle differently based on your needs\n",
        "\n",
        "# 'data' now contains a list of decoded JSON objects from the file\n",
        "for item in data:\n",
        "    print(json.dumps(item, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggt62xDgLrlz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata = pd.read_csv('images/metadata/images.csv')\n",
        "metadata.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWR4xHcncyAs"
      },
      "outputs": [],
      "source": [
        "id_path_dict = {}\n",
        "for index, row in metadata.iterrows():\n",
        "    id_path_dict[row['image_id']] = row['path']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElcxANYW2nVw"
      },
      "outputs": [],
      "source": [
        "([data[0][\"main_image_id\"]] + data[0][\"other_image_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JEoC7n0LxvS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "images = [Image.open('/content/images/small/' + id_path_dict[id]) for id in ([data[0][\"main_image_id\"]] + data[0][\"other_image_id\"])]\n",
        "for img in images:\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghOSasUkPSbS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def QAParser(qa_text):\n",
        "    # clean it to start from ```json and end with ```\n",
        "    qa_text = qa_text.split(\"```json\")[1]\n",
        "    qa_text = qa_text.split(\"```\")[0]\n",
        "    # convert string to json\n",
        "    qa_json = json.loads(qa_text)\n",
        "    return qa_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFOKthe6_H9H"
      },
      "outputs": [],
      "source": [
        "# # prompts:\n",
        "\n",
        "# content_raw = [\n",
        "#         images,\n",
        "#         \"Generate around 20 diverse questions about this item and its metadata, each followed by a single-word (valid words) answer \\\n",
        "#         and a difficulty category ('easy', 'medium', or 'hard').\\\n",
        "#         Return the result as a JSON array \\\n",
        "#         of dictionaries with the keys 'question', 'answer', and 'category'.\\\n",
        "#         \"\n",
        "#     ]\n",
        "\n",
        "# content_filtered = [\n",
        "#         images,\n",
        "#         json.dumps(data[0]),\n",
        "#         json.dumps(parsed_qas),\n",
        "#         \"Filter out the incorrect question answers based on the metadata and images provided and provide 5 confidently correct question and answers.\\\n",
        "#         Provide the output in a json format.\"\n",
        "#     ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZUqhm7-jnP"
      },
      "source": [
        "#### Using GEMINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRD8MUZdMFO7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "GEMINI_API_KEY = [\n",
        "    # insert your API Keys here\n",
        "]\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client_arr = [genai.Client(api_key=i) for i in GEMINI_API_KEY]\n",
        "client = client_arr[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biBjzgrGN68v"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=[\n",
        "        images,\n",
        "        \"You are shown a set of related images and associated metadata.\\\n",
        "        Your task is to generate question-answer pairs that are:\\\n",
        "        - Answerable from **only one or a few images**\\\n",
        "        - Variety in qas\\\n",
        "        Return the result as a JSON array \\\n",
        "        of dictionaries with the keys 'question', 'answer', and 'category {easy, medium or hard}'.\\\n",
        "        \"\n",
        "    ]\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDuyl-TY7w-E"
      },
      "outputs": [],
      "source": [
        "parsed_qas = QAParser(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx3ppQDf7aAq"
      },
      "outputs": [],
      "source": [
        "# . Refrain from making as many assumptions as possible\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=[\n",
        "        images,\n",
        "        json.dumps(data[0]),\n",
        "        json.dumps(parsed_qas),\n",
        "        \"Filter out the incorrect question answers based on the metadata and images provided and provide 5 confidently correct question and answers.\\\n",
        "        Provide the output in a json format.\"\n",
        "    ]\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzQyq7zp-opX"
      },
      "source": [
        "#### Using Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZuMzCOK-qq3"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR7tt_6F-x_Y"
      },
      "outputs": [],
      "source": [
        "!nohup ollama serve &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-aBTQv4-y3D"
      },
      "outputs": [],
      "source": [
        "!ollama pull llava:5b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I21StLyjYcKk"
      },
      "outputs": [],
      "source": [
        "!curl http://localhost:11434/api/tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk340mKk-3b5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def query_ollama(model, prompt, images=None):\n",
        "    \"\"\"Queries the Ollama server with the given prompt and base64-encoded images.\"\"\"\n",
        "    if images is None:\n",
        "        images = []\n",
        "\n",
        "    encoded_images = []\n",
        "    for img in images:\n",
        "        buffered = BytesIO()\n",
        "        img.save(buffered, format=\"JPEG\")\n",
        "        encoded = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "        encoded_images.append(encoded)\n",
        "\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"images\": encoded_images,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    resp = requests.post(url, json=payload)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json().get('response', '')\n",
        "\n",
        "# Load and show images\n",
        "image_ids = [data[0][\"main_image_id\"]] #+ data[0].get(\"other_image_id\", [])\n",
        "images = []\n",
        "for img_id in image_ids:\n",
        "    img_path = f\"/content/images/small/{id_path_dict[img_id]}\"\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        images.append(img)\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't load image {img_id}: {e}\")\n",
        "\n",
        "for img in images:\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Use metadata to guide QA generation\n",
        "metadata = data[0]\n",
        "model_name = \"llava:7b\"\n",
        "\n",
        "# for img_id, img in images:\n",
        "prompt = f\"\"\"\n",
        "You are given 1–3 product images and a short metadata description.\n",
        "Your task is to generate **5 question–answer pairs** to train a visual question answering (VQA) model.\n",
        "Constraints:\n",
        "1. Each answer must be **visually inferable** — do not rely on metadata.\n",
        "2. Each question should be **inspired by metadata**, but **must be answerable from the image(s) alone**.\n",
        "3. Each answer must be a **single English word** (alphabetical only).\n",
        "4. Cover **varied aspects**: color, material, shape, style, components, etc.\n",
        "5. Label each question as either: \"easy\", \"medium\", or \"hard\" (w.r.t. model effort).\n",
        "Metadata (not visible to the model at inference time): {metadata.get('item_keywords', item)}\n",
        "Return only valid JSON as a list of dictionaries:\n",
        "[\n",
        "{{ \"question\": \"...\", \"answer\": \"...\", \"category\": \"easy\" }},\n",
        "...\n",
        "]\n",
        "\"\"\"\n",
        "try:\n",
        "    result = query_ollama(model_name, prompt, images)\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to query image {img_id}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ZM7Aa9ADDo"
      },
      "source": [
        "#### Driver code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL5S1ZUyBsm5"
      },
      "outputs": [],
      "source": [
        "# index of listings.json\n",
        "i = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdF8NGHkACbh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the file content\n",
        "with open(f'/content/listings/metadata/listings_{i}.json', 'r') as f:\n",
        "    file_content = f.read()\n",
        "\n",
        "# Attempt to decode JSON objects iteratively\n",
        "data = []\n",
        "decoder = json.JSONDecoder()\n",
        "while file_content:\n",
        "    try:\n",
        "        obj, index = decoder.raw_decode(file_content)\n",
        "        data.append(obj)\n",
        "        file_content = file_content[index:].lstrip()  # Remove processed data and leading whitespace\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Handle potential errors, e.g., log them or break the loop\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "        break  # Or handle differently based on your needs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "kpYo9ZH7PTik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddkSpXB3AzWs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ci = 0\n",
        "client = client_arr[ci]\n",
        "\n",
        "file_name = f'/content/dataset_{i}.csv'\n",
        "dataset = []\n",
        "# load the existing one\n",
        "flag = True\n",
        "if os.path.exists(file_name) and flag:\n",
        "    dataset = pd.read_csv(file_name).to_dict('records')\n",
        "    print(f\"Loaded {len(dataset)} items from {file_name}\")\n",
        "\n",
        "\n",
        "def QAParser(response_text):\n",
        "    try:\n",
        "        # Remove markdown code fences (```json ... ```)\n",
        "        text = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", response_text.strip(), flags=re.IGNORECASE)\n",
        "\n",
        "        # Extract the first valid JSON array\n",
        "        match = re.search(r'(\\[\\s*\\{.*?\\}\\s*\\])', text, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No valid JSON array found.\")\n",
        "        arr = match.group(1)\n",
        "\n",
        "        # Fix missing commas if needed\n",
        "        arr = re.sub(r'(\"answer\":\\s*\"[^\"]+\")\\s*(\"category\":)', r'\\1, \\2', arr)\n",
        "\n",
        "        qas = json.loads(arr)\n",
        "\n",
        "        # Validate and sanitize\n",
        "        valid = []\n",
        "        for qa in qas:\n",
        "            if all(k in qa for k in (\"question\", \"answer\", \"category\")):\n",
        "                valid.append({\n",
        "                    \"question\": qa[\"question\"].strip(),\n",
        "                    \"answer\": qa[\"answer\"].strip().split()[0].capitalize(),\n",
        "                    \"category\": qa[\"category\"].strip().lower()\n",
        "                })\n",
        "        return valid\n",
        "    except Exception as e:\n",
        "        print(f\"[Parsing Error] {e}\")\n",
        "        print(\"[Raw]\", repr(response_text)[:200])\n",
        "        return []\n",
        "\n",
        "\n",
        "def query_ollama(model, prompt, images=None):\n",
        "    if images is None:\n",
        "        images = []\n",
        "    encoded = []\n",
        "    for img in images:\n",
        "        buf = BytesIO()\n",
        "        img.save(buf, format=\"JPEG\")\n",
        "        encoded.append(base64.b64encode(buf.getvalue()).decode())\n",
        "    resp = requests.post(\n",
        "        \"http://localhost:11434/api/generate\",\n",
        "        json={\"model\": model, \"prompt\": prompt, \"images\": encoded, \"stream\": False}\n",
        "    )\n",
        "    resp.raise_for_status()\n",
        "    return resp.json().get(\"response\", \"\")\n",
        "\n",
        "def tryPromptGemini(imgs, item):\n",
        "    resp = None\n",
        "    global client\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=[\n",
        "                imgs,\n",
        "                f\"\"\"\n",
        "You are given 1–3 product images and a short metadata description.\n",
        "Your task is to generate **5 question–answer pairs** to train a visual question answering (VQA) model.\n",
        "Constraints:\n",
        "1. Each answer must be **visually inferable** — do not rely on metadata.\n",
        "2. Each question should be **inspired by metadata**, but **must be answerable from the image(s) alone**.\n",
        "3. Each answer must be a **single English word** (alphabetical only).\n",
        "4. Cover **varied aspects**: color, material, shape, style, components, etc.\n",
        "5. Label each question as either: \"easy\", \"medium\", or \"hard\" (w.r.t. model effort).\n",
        "Metadata (not visible to the model at inference time): {item.get('item_keywords', item)}\n",
        "Return only valid JSON as a list of dictionaries:\n",
        "[\n",
        "{{ \"question\": \"...\", \"answer\": \"...\", \"category\": \"easy\" }},\n",
        "...\n",
        "]\n",
        "\"\"\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        resp = response.text\n",
        "    except Exception as e:\n",
        "        global ci\n",
        "        ci = (ci + 1) % len(client_arr)\n",
        "        client = client_arr[ci]\n",
        "        wait = 10\n",
        "        # error_msg = str(e)\n",
        "        # if \"RESOURCE_EXHAUSTED\" in error_msg:\n",
        "        #     wait = 120  # longer wait for quota errors\n",
        "        # elif \"UNAVAILABLE\" in error_msg:\n",
        "        #     wait = 30\n",
        "        # else:\n",
        "        #     wait = 10\n",
        "\n",
        "        print(f\"[Error] {e}, redoing after {wait} seconds\")\n",
        "        time.sleep(wait)\n",
        "        tryPromptGemini(imgs, item)\n",
        "    return resp\n",
        "\n",
        "# Process items\n",
        "Skippings = []\n",
        "df = pd.DataFrame()\n",
        "for idx, item in enumerate(tqdm(data, desc=\"Processing items\")):\n",
        "    if idx < len(dataset):\n",
        "        continue\n",
        "    imgs = []\n",
        "    for img_id in [item.get(\"main_image_id\")]: # + item.get(\"other_image_id\", []):\n",
        "        if img_id is None: continue\n",
        "        path = f'/content/images/small/{id_path_dict[img_id]}'\n",
        "        try:\n",
        "            imgs.append(Image.open(path))\n",
        "        except:\n",
        "            continue\n",
        "    if not imgs:\n",
        "        continue\n",
        "\n",
        "    # # show the iamge\n",
        "    # for img in imgs:\n",
        "    #     plt.imshow(img)\n",
        "    #     plt.axis('off')\n",
        "    #     plt.show()\n",
        "\n",
        "    resp = tryPromptGemini(imgs, item)\n",
        "\n",
        "    if resp is None:\n",
        "        Skippings.append(item)\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        qas = QAParser(resp)\n",
        "        if not qas:\n",
        "            print(json.dumps(qas, indent=4))\n",
        "            continue\n",
        "        # print(json.dumps(qas, indent=4))\n",
        "        dataset.append({\n",
        "            \"item_id\": item[\"item_id\"],\n",
        "            \"qas\": qas,\n",
        "            \"image_id\": item[\"main_image_id\"]\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {e}\")\n",
        "        continue\n",
        "    if len(dataset) % 50 == 0:\n",
        "        pd.DataFrame(dataset).to_csv(file_name, index=False)\n",
        "\n",
        "# Save\n",
        "pd.DataFrame(dataset).to_csv(file_name, index=False)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_LCB9W2l6Xo"
      },
      "outputs": [],
      "source": [
        "file_name = f'/content/dataset_{i}.csv'\n",
        "dataset = []\n",
        "# load the existing one\n",
        "if os.path.exists(file_name):\n",
        "    dataset = pd.read_csv(file_name).to_dict('records')\n",
        "    print(f\"Loaded {len(dataset)} items from {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNklWB90w9yj"
      },
      "outputs": [],
      "source": [
        "# download the dataset to system\n",
        "from google.colab import files\n",
        "files.download(f\"/content/dataset_{i}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yPi2phaPeWP"
      },
      "outputs": [],
      "source": [
        "# bert's code\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "            #     model=\"gemini-2.0-flash\",\n",
        "                # contents=[\n",
        "                #     images,\n",
        "                #     json.dumps(item),\n",
        "                #     json.dumps(parsed_qas),\n",
        "                #     \"You have:\\n\"\n",
        "                #     \"- Product images\\n\"\n",
        "                #     \"- Metadata JSON\\n\"\n",
        "                #     \"- 20 generated Q&A pairs\\n\\n\"\n",
        "                #     \"Task: From those, select 5 that are clearly and confidently answerable using the image and metadata alone.\\n\"\n",
        "                #     \"Only include Q&A pairs with correct, unambiguous, single-word answers.\\n\"\n",
        "                #     \"Return a JSON array of 5 objects with keys 'question', 'answer', and 'category'.\"\n",
        "                # ]\n",
        "            # )\n",
        "\n",
        "# # Prompt 2: QA Filtering (Only for Ollama)\n",
        "        # #                     \"The reselt of this will be used to train and evaluate vision language models solely based on the images.\\n\"\n",
        "\n",
        "        # try:\n",
        "\n",
        "        #     response = query_ollama(\n",
        "        #         model=\"llava:7b\",\n",
        "        #         prompt=(\n",
        "        #             \"You have:\\n\"\n",
        "        #             \"- Product image\\n\"\n",
        "        #             f\"- Metadata JSON:\\n{json.dumps(item, indent=2)}\\n\"\n",
        "        #             f\"- generated Q&A pairs:\\n{json.dumps(parsed_qas, indent=2)}\\n\\n\"\n",
        "        #             \"Task: From those, filter 5 that are clearly and confidently answerable using the image and metadata alone.\\n\"\n",
        "        #             \"The QAs must be general for all images and not specific to any one.\"\n",
        "        #             \"Only filter Q&A pairs with correct, unambiguous, single-word answers based on the metadata and images.\\n\"\n",
        "        #             \"Do not generate own QAs\"\n",
        "        #             \"Return a JSON array of 5 objects with keys 'question', 'answer', and 'category' ie, ```json <message> ``` \\n\"\n",
        "        #         ),\n",
        "        #         images=images\n",
        "        #     )\n",
        "\n",
        "\n",
        "        #     filtered_qas = QAParser(response)\n",
        "        #     # print(json.dumps(parsed_qas, indent=4))\n",
        "        #     print(json.dumps(filtered_qas, indent=4))\n",
        "        # except Exception as e:\n",
        "        #     print(f\"[Filter Error] Skipping filtering for item {item['item_id']}: {e}\")\n",
        "        #     continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSqLt7YQyOg0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gJXzRKuX3E6"
      },
      "source": [
        "## Evaluating Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf317MCPZ3X6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate pillow\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUb5KSxYYE4s"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "i = '7'\n",
        "import pandas as pd\n",
        "eval_dataset = pd.read_csv(f'/content/dataset_{i}.csv').to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONkfPx9UYQYb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from bert_score import score\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the BLIP VQA model\n",
        "# Using the smaller 'Salesforce/blip-vqa-base' for faster evaluation\n",
        "vqa_pipeline = pipeline(\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Prepare data for evaluation\n",
        "eval_results = []\n",
        "for item_data in tqdm(eval_dataset, desc=\"Evaluating with BLIP\"):\n",
        "    item_id = item_data['item_id']\n",
        "    image_id = item_data['image_id']\n",
        "    # Use ast.literal_eval to safely parse the string representation of the list\n",
        "    try:\n",
        "        qas = ast.literal_eval(item_data['qas'])\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        print(f\"Error parsing QAs for item {item_id}: {e}\")\n",
        "        print(f\"Problematic string: {item_data['qas']}\")\n",
        "        continue\n",
        "\n",
        "    # Load the image\n",
        "    img_path = f'/content/images/small/{id_path_dict[image_id]}'\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load image {image_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # # show the image\n",
        "    # plt.imshow(img)\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "    # Evaluate each Q&A pair\n",
        "    for qa in qas:\n",
        "        question = qa['question']\n",
        "        ground_truth_answer = qa['answer']\n",
        "        category = qa['category']\n",
        "\n",
        "        try:\n",
        "            # Get prediction from BLIP\n",
        "            prediction = vqa_pipeline(image=img, question=question)\n",
        "            # The output is a list of dicts, take the answer with the highest score\n",
        "            predicted_answer = prediction[0]['answer']\n",
        "            # print(\"Question:\", question)\n",
        "            # print(\"Ground Truth Answer:\", ground_truth_answer)\n",
        "            # print(\"Predicted Answer:\", predicted_answer)\n",
        "            # print()\n",
        "            # Store results\n",
        "            eval_results.append({\n",
        "                \"item_id\": item_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"question\": question,\n",
        "                \"ground_truth_answer\": ground_truth_answer,\n",
        "                \"predicted_answer_blip\": predicted_answer,\n",
        "                \"category\": category\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question for item {item_id}: {e}\")\n",
        "            eval_results.append({\n",
        "                \"item_id\": item_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"question\": question,\n",
        "                \"ground_truth_answer\": ground_truth_answer,\n",
        "                \"predicted_answer_blip\": \"ERROR\",\n",
        "                \"category\": category\n",
        "            })\n",
        "\n",
        "\n",
        "# Analyze results (basic accuracy)\n",
        "eval_df = pd.DataFrame(eval_results)\n",
        "\n",
        "# Use direct matching loss\n",
        "eval_df['exact_match'] = eval_df['predicted_answer_blip'] == eval_df['ground_truth_answer']\n",
        "\n",
        "# Combine question and answer to give context\n",
        "preds = (eval_df['question'] + \" \" + eval_df['predicted_answer_blip']).astype(str).tolist()\n",
        "refs = (eval_df['question'] + \" \" + eval_df['ground_truth_answer']).astype(str).tolist()\n",
        "\n",
        "# Compute BERTScore\n",
        "P, R, F1 = score(preds, refs, lang=\"en\", verbose=True)\n",
        "\n",
        "# Store results\n",
        "eval_df['bertscore_f1_with_question'] = F1.tolist()\n",
        "\n",
        "print(f\"\\nBLIP VQA Score:\\nPrecision={P.mean():.4f}, Recall={R.mean():.4f}, F1={F1.mean():.4f}\")\n",
        "\n",
        "# Optionally, save the evaluation results\n",
        "eval_output_filename = f'/content/blip_eval_results_{i}.csv'\n",
        "eval_df.to_csv(eval_output_filename, index=False)\n",
        "print(f\"Evaluation results saved to {eval_output_filename}\")\n",
        "\n",
        "# Display some sample results\n",
        "print(\"\\nSample Evaluation Results:\")\n",
        "print(eval_df.head())\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Normalize answers\n",
        "eval_df['ground_truth_answer_clean'] = eval_df['ground_truth_answer'].str.lower().str.strip()\n",
        "eval_df['predicted_answer_blip_clean'] = eval_df['predicted_answer_blip'].str.lower().str.strip()\n",
        "\n",
        "# Exact string match\n",
        "eval_df['exact_match'] = eval_df['predicted_answer_blip_clean'] == eval_df['ground_truth_answer_clean']\n",
        "\n",
        "# Accuracy (proportion of correct answers)\n",
        "accuracy = eval_df['exact_match'].mean()\n",
        "print(f\"\\nExact Match Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Binary values for precision, recall, F1\n",
        "y_true = eval_df['ground_truth_answer_clean'] == eval_df['ground_truth_answer_clean']  # all True\n",
        "y_pred = eval_df['exact_match']\n",
        "\n",
        "# Precision, Recall, F1 based on exact match (1 if correct, 0 if incorrect)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1_direct = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Direct Match F1 Score:\\nPrecision={precision:.4f}, Recall={recall:.4f}, F1={f1_direct:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIddWKiCZy4p"
      },
      "outputs": [],
      "source": [
        "# Uninstall existing torch installation\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "# Install a specific version of torch and torchvision that should be compatible\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Reinstall the required libraries\n",
        "!pip install transformers accelerate pillow bert_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6R1VOERCuHq"
      },
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8As57kDagWX"
      },
      "outputs": [],
      "source": [
        "!pip install peft accelerate transformers datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering, Trainer, TrainingArguments\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "BASE_IMAGE_PATH = 'images/small'  # Adjust this to match your images/small directory\n",
        "CSV_PATH = 'merged.csv'  # Path to your CSV file\n",
        "METADATA_PATH = 'images/metadata/images.csv'  # Path to the metadata CSV\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Blip\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", use_fast=True)\n",
        "\n",
        "# # Smol-256\n",
        "# processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n",
        "# model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\").to(device)\n",
        "\n",
        "\n",
        "# === LOAD YOUR CURATED CSV ===\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(f\"Loaded custom dataset with {len(df)} entries.\")\n",
        "\n",
        "# Load image metadata to map image_id to file paths\n",
        "try:\n",
        "    metadata_df = pd.read_csv(METADATA_PATH)\n",
        "    print(f\"Loaded metadata with {len(metadata_df)} images.\")\n",
        "    # Create a mapping from image_id to path\n",
        "    image_id_to_path = {}\n",
        "    for _, row in metadata_df.iterrows():\n",
        "        if 'image_id' in row and 'path' in row:\n",
        "            image_id_to_path[row['image_id']] = row['path']\n",
        "except Exception as e:\n",
        "    print(f\"Error loading metadata: {e}\")\n",
        "    # Fallback: assume image_id directly maps to path\n",
        "    image_id_to_path = {}\n"
      ],
      "metadata": {
        "id": "GOmBZyoP9Es6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    image_id = row['image_id']\n",
        "    try:\n",
        "        qas_list = ast.literal_eval(row['qas'])  # Safer than eval, accepts Python-style lists\n",
        "        for qa in qas_list:\n",
        "            question = qa.get('question', '').strip()\n",
        "            answer = qa.get('answer', '').strip()\n",
        "            rows.append({\n",
        "                'image_id': image_id,\n",
        "                'question': question,\n",
        "                'answer': answer\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to parse qas for image_id {image_id}: {e}\")\n",
        "\n",
        "flattened_df = pd.DataFrame(rows)\n",
        "\n",
        "# Only run this if there are actually rows\n",
        "if not flattened_df.empty:\n",
        "    flattened_df['answer'] = flattened_df['answer'].fillna('unknown').astype(str)\n",
        "    flattened_df['image_id'] = flattened_df['image_id'].astype(str)\n",
        "    print(flattened_df.head())\n",
        "else:\n",
        "    print(\"No valid QAs parsed. Please check the input format.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ta18VhSH91E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take subset of dataset\n",
        "train_df = flattened_df.sample(frac=0.1, random_state=42)\n",
        "eval_df = flattened_df.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Bi84XFDy-Ete"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================ ACCELERATOR INIT ==================== BLIP ===================================\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Ensure proper types\n",
        "train_df['answer'] = train_df['answer'].fillna('unknown').astype(str)\n",
        "train_df['image_id'] = train_df['image_id'].astype(str)\n",
        "\n",
        "# === TRAIN-TEST SPLIT ===\n",
        "print(f\"Train size: {len(train_df)}\")# | Test size: {len(test_df)}\")\n",
        "\n",
        "# === DEFINE CUSTOM DATASET ===\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, processor, image_base_path, image_id_to_path=None):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.image_base_path = image_base_path\n",
        "        self.image_id_to_path = image_id_to_path or {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_id = row['image_id']\n",
        "\n",
        "        # Try to find image path using metadata mapping\n",
        "        if image_id in self.image_id_to_path:\n",
        "            # Use the path from metadata\n",
        "            relative_path = self.image_id_to_path[image_id]\n",
        "            full_image_path = os.path.join(self.image_base_path, relative_path)\n",
        "        else:\n",
        "            # Fallback: Determine path based on image_id first two characters\n",
        "            # For example, if image_id is \"81iZlv3bjpL\", it would go in folder \"8\"\n",
        "            # Adjust this logic based on your actual naming convention\n",
        "            prefix = image_id[:2]\n",
        "            full_image_path = os.path.join(self.image_base_path, prefix, f\"{image_id}.jpg\")\n",
        "\n",
        "            # If not found, try alternative patterns\n",
        "            if not os.path.exists(full_image_path):\n",
        "                # Try looking in a folder matching the first two characters\n",
        "                prefix = image_id[:2]\n",
        "                full_image_path = os.path.join(self.image_base_path, prefix, f\"{image_id}.jpg\")\n",
        "\n",
        "            if not os.path.exists(full_image_path):\n",
        "                # Last resort: search for the image recursively (could be slow)\n",
        "                for root, _, files in os.walk(self.image_base_path):\n",
        "                    for file in files:\n",
        "                        if image_id in file:\n",
        "                            full_image_path = os.path.join(root, file)\n",
        "                            break\n",
        "\n",
        "        try:\n",
        "            image = Image.open(full_image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {full_image_path} for image_id {image_id}: {e}\")\n",
        "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))  # Fallback image\n",
        "\n",
        "        encoding = self.processor(\n",
        "            images=image,\n",
        "            text=row['question'],\n",
        "            padding=\"max_length\",\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        labels = self.processor.tokenizer(\n",
        "            row['answer'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=32,\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        encoding[\"labels\"] = labels.squeeze(0)\n",
        "        return encoding\n",
        "\n",
        "# Add verification functions here\n",
        "def verify_dataset_images(dataset, num_samples=5):\n",
        "    \"\"\"Verify that images are being loaded correctly by checking a few samples\"\"\"\n",
        "    print(\"\\n=== DATASET VERIFICATION ===\")\n",
        "    print(f\"Dataset contains {len(dataset)} samples\")\n",
        "\n",
        "    # Check a few random samples\n",
        "    import random\n",
        "    random.seed(42)  # For reproducibility\n",
        "    sample_indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        try:\n",
        "            # Get the original data row\n",
        "            row = dataset.df.iloc[idx]\n",
        "            print(f\"\\nSample {i+1}/{len(sample_indices)}:\")\n",
        "            print(f\"  Question: {row['question']}\")\n",
        "            print(f\"  Answer: {row['answer']}\")\n",
        "            print(f\"  Image ID: {row['image_id']}\")\n",
        "\n",
        "            # Try to get the processed item\n",
        "            item = dataset[idx]\n",
        "            if 'pixel_values' in item:\n",
        "                pixel_shape = item['pixel_values'].shape\n",
        "                print(f\"  Image loaded successfully with shape: {pixel_shape}\")\n",
        "            else:\n",
        "                print(\"  Warning: No pixel_values in processed item\")\n",
        "\n",
        "            if 'input_ids' in item:\n",
        "                input_length = item['input_ids'].shape[0]\n",
        "                print(f\"  Question tokenized to {input_length} tokens\")\n",
        "            else:\n",
        "                print(\"  Warning: No input_ids in processed item\")\n",
        "\n",
        "            if 'labels' in item:\n",
        "                label_length = item['labels'].shape[0]\n",
        "                print(f\"  Answer tokenized to {label_length} tokens\")\n",
        "            else:\n",
        "                print(\"  Warning: No labels in processed item\")\n",
        "\n",
        "            print(\"  Sample loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing sample {idx}: {e}\")\n",
        "\n",
        "    print(\"\\n=== VERIFICATION COMPLETE ===\\n\")\n",
        "    return True\n",
        "\n",
        "# === CREATE DATASET INSTANCE ===\n",
        "train_dataset = VQADataset(train_df, processor, BASE_IMAGE_PATH, image_id_to_path)\n",
        "\n",
        "# Verify that the dataset is working properly\n",
        "verify_dataset_images(train_dataset)\n",
        "\n",
        "# === APPLY LoRA TO MODEL ===\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied.\")\n",
        "\n",
        "# === PREPARE MODEL FOR ACCELERATION ===\n",
        "model = accelerator.prepare(model)\n",
        "\n",
        "# === DEFINE TRAINING ARGUMENTS ===\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"blip_vqa_lora_finetune_curated\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# === DEFINE A VALIDATION CALLBACK ===\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class ValidationCallback(TrainerCallback):\n",
        "    def __init__(self, processor, interval=500):\n",
        "        self.processor = processor\n",
        "        self.interval = interval\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % self.interval == 0 and state.global_step > 0:\n",
        "            model = kwargs.get('model', None)\n",
        "            if model is None:\n",
        "                return\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Generate a prediction for a simple example\n",
        "                prompt = \"What color is the object in the image?\"\n",
        "                inputs = processor(images=Image.new(\"RGB\", (224, 224), (100, 150, 200)),\n",
        "                                  text=prompt, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Generate output\n",
        "                generated_ids = model.generate(**inputs, max_length=20)\n",
        "                generated_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "                print(f\"\\n=== VALIDATION AT STEP {state.global_step} ===\")\n",
        "                print(f\"Q: {prompt}\")\n",
        "                print(f\"A: {generated_text}\")\n",
        "                print(f\"Current training loss: {state.log_history[-1]['loss']:.4f}\")\n",
        "                print(f\"=== END VALIDATION ===\\n\")\n",
        "\n",
        "            model.train()\n",
        "\n",
        "# === TRAINER SETUP ===\n",
        "validation_callback = ValidationCallback(processor)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    callbacks=[validation_callback]\n",
        ")\n",
        "\n",
        "# # === GPU INFO ===\n",
        "# if torch.cuda.is_available():\n",
        "#     print(\"GPU GPU Memory Usage Before Training:\")\n",
        "#     print(torch.cuda.memory_summary())\n",
        "\n",
        "# # === START TRAINING ===\n",
        "# trainer.train()\n",
        "\n",
        "# # === SAVE MODEL ===\n",
        "# trainer.save_model(\"./blip_vqa_lora_r_16\")\n",
        "# print(\"Model saved to './blip_vqa_lora_r_16'\")\n"
      ],
      "metadata": {
        "id": "BsnNkTTZ7P8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ========================= SMOL PART (IGNORE) ======================================\n",
        "\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# from PIL import Image\n",
        "# from transformers import BlipProcessor, BlipForQuestionAnswering, Trainer, TrainingArguments\n",
        "# from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "# import torch\n",
        "# from accelerate import Accelerator\n",
        "# from peft import LoraConfig, get_peft_model\n",
        "# from transformers.data.data_collator import default_data_collator\n",
        "# import ast\n",
        "# import re\n",
        "# import time\n",
        "# from tqdm import tqdm\n",
        "# import requests\n",
        "# import base64\n",
        "# from io import BytesIO\n",
        "# import matplotlib.pyplot as plt\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# from transformers.data.data_collator import DataCollatorMixin\n",
        "# from transformers import TrainerCallback\n",
        "\n",
        "\n",
        "# # === ACCELERATOR INIT ===\n",
        "# accelerator = Accelerator()\n",
        "\n",
        "# # Ensure proper types\n",
        "# train_df['answer'] = train_df['answer'].fillna('unknown').astype(str)\n",
        "# train_df['image_id'] = train_df['image_id'].astype(str)\n",
        "\n",
        "# # === TRAIN-TEST SPLIT ===\n",
        "# print(f\"Train size: {len(train_df)}\")# | Test size: {len(test_df)}\")\n",
        "\n",
        "# # === DEFINE CUSTOM DATASET ===\n",
        "# class VQADataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, df, processor, image_base_path, image_id_to_path=None):\n",
        "#         self.df = df\n",
        "#         self.processor = processor\n",
        "#         self.image_base_path = image_base_path\n",
        "#         self.image_id_to_path = image_id_to_path or {}\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "#         image_id = str(row['image_id'])\n",
        "#         question = row['question'] or \"\"\n",
        "#         answer   = row['answer'] or \"\"\n",
        "\n",
        "#         # 1) Locate & load the image\n",
        "#         if image_id in self.image_id_to_path:\n",
        "#             rel = self.image_id_to_path[image_id]\n",
        "#             img_path = os.path.join(self.image_base_path, rel)\n",
        "#         else:\n",
        "#             prefix = image_id[:2]\n",
        "#             img_path = os.path.join(self.image_base_path, prefix, f\"{image_id}.jpg\")\n",
        "#             if not os.path.exists(img_path):\n",
        "#                 for root, _, files in os.walk(self.image_base_path):\n",
        "#                     for fname in files:\n",
        "#                         if image_id in fname:\n",
        "#                             img_path = os.path.join(root, fname)\n",
        "#                             break\n",
        "\n",
        "#         try:\n",
        "#             image = Image.open(img_path).convert(\"RGB\")\n",
        "#         except:\n",
        "#             # fallback black image\n",
        "#             # Use a reasonable default size, e.g., 224x224 or 512x512 based on typical VLM inputs\n",
        "#             image = Image.new(\"RGB\", (512, 512), (0, 0, 0))\n",
        "\n",
        "#         # 2) Ensure prompt has one <image> token\n",
        "#         prompt = f\"<image> {question.strip()}\"\n",
        "\n",
        "#         # 3) Encode image + text together\n",
        "#         # The processor handles resizing/patching. We return the resulting tensor.\n",
        "#         encoding = self.processor(\n",
        "#             images=image,\n",
        "#             text=prompt,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             max_length=128,\n",
        "#             return_tensors=\"pt\",\n",
        "#             return_attention_mask=True\n",
        "#         )\n",
        "\n",
        "#         # 4) Encode the answer separately as labels\n",
        "#         labels = self.processor.tokenizer(\n",
        "#             answer,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             max_length=32,\n",
        "#             return_tensors=\"pt\"\n",
        "#         )[\"input_ids\"]\n",
        "\n",
        "#         # 5) Squeeze batch dimension\n",
        "#         item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "#         item[\"labels\"] = labels.squeeze(0)\n",
        "\n",
        "#         return item\n",
        "\n",
        "\n",
        "# # Add verification functions here\n",
        "# def verify_dataset_images(dataset, num_samples=5):\n",
        "#     \"\"\"Verify that images are being loaded correctly by checking a few samples\"\"\"\n",
        "#     print(\"\\n=== DATASET VERIFICATION ===\")\n",
        "#     print(f\"Dataset contains {len(dataset)} samples\")\n",
        "\n",
        "#     # Check a few random samples\n",
        "#     import random\n",
        "#     random.seed(42)  # For reproducibility\n",
        "#     sample_indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "#     for i, idx in enumerate(sample_indices):\n",
        "#         try:\n",
        "#             # Get the original data row\n",
        "#             row = dataset.df.iloc[idx]\n",
        "#             print(f\"\\nSample {i+1}/{len(sample_indices)}:\")\n",
        "#             print(f\"  Question: {row['question']}\")\n",
        "#             print(f\"  Answer: {row['answer']}\")\n",
        "#             print(f\"  Image ID: {row['image_id']}\")\n",
        "\n",
        "#             # Try to get the processed item\n",
        "#             item = dataset[idx]\n",
        "#             if 'pixel_values' in item:\n",
        "#                 pixel_shape = item['pixel_values'].shape\n",
        "#                 print(f\"  Image loaded successfully with shape: {pixel_shape}\")\n",
        "#             else:\n",
        "#                 print(\"  Warning: No pixel_values in processed item\")\n",
        "\n",
        "#             if 'input_ids' in item:\n",
        "#                 input_length = item['input_ids'].shape[0]\n",
        "#                 print(f\"  Question tokenized to {input_length} tokens\")\n",
        "#             else:\n",
        "#                 print(\"  Warning: No input_ids in processed item\")\n",
        "\n",
        "#             if 'labels' in item:\n",
        "#                 label_length = item['labels'].shape[0]\n",
        "#                 print(f\"  Answer tokenized to {label_length} tokens\")\n",
        "#             else:\n",
        "#                 print(\"  Warning: No labels in processed item\")\n",
        "\n",
        "#             print(\"  Sample loaded successfully!\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"  Error processing sample {idx}: {e}\")\n",
        "\n",
        "#     print(\"\\n=== VERIFICATION COMPLETE ===\\n\")\n",
        "#     return True\n",
        "\n",
        "# # === CREATE DATASET INSTANCE ===\n",
        "# train_dataset = VQADataset(train_df, processor, BASE_IMAGE_PATH, image_id_to_path)\n",
        "\n",
        "# # Verify that the dataset is working properly\n",
        "# verify_dataset_images(train_dataset)\n",
        "\n",
        "# # === APPLY LoRA TO MODEL ===\n",
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"v_proj\"], # for Smol\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\"\n",
        "# )\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# print(\"LoRA applied.\")\n",
        "\n",
        "# # === PREPARE MODEL FOR ACCELERATION ===\n",
        "# model = accelerator.prepare(model)\n",
        "\n",
        "# # === DEFINE TRAINING ARGUMENTS ===\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     run_name=\"smol256_vqa_lora_finetune_curated\", # Changed run name\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=1,\n",
        "#     learning_rate=5e-4,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir='./logs',\n",
        "#     logging_steps=10,\n",
        "#     save_strategy=\"epoch\",\n",
        "#     fp16=True,\n",
        "#     remove_unused_columns=False,\n",
        "#     report_to=\"none\"\n",
        "# )\n",
        "\n",
        "\n",
        "# # === DEFINE COLLATOR ===\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# from transformers.data.data_collator import default_data_collator\n",
        "\n",
        "# class VQACollator:\n",
        "#     def __init__(self, processor):\n",
        "#         self.processor = processor\n",
        "\n",
        "#     def __call__(self, features):\n",
        "#         # 1) Stack all pixel_values into (batch, C, H, W)\n",
        "#         pixel_values = torch.stack([f.pop(\"pixel_values\") for f in features])\n",
        "\n",
        "#         # 2) Pad input_ids and attention_mask to the same length in the batch\n",
        "#         input_ids      = pad_sequence(\n",
        "#                              [f.pop(\"input_ids\")      for f in features],\n",
        "#                              batch_first=True,\n",
        "#                              padding_value=self.processor.tokenizer.pad_token_id\n",
        "#                          )\n",
        "#         attention_mask = pad_sequence(\n",
        "#                              [f.pop(\"attention_mask\") for f in features],\n",
        "#                              batch_first=True,\n",
        "#                              padding_value=0\n",
        "#                          )\n",
        "\n",
        "#         # 3) Pad labels (and use -100 to ignore them in loss)\n",
        "#         labels = pad_sequence(\n",
        "#                      [f.pop(\"labels\") for f in features],\n",
        "#                      batch_first=True,\n",
        "#                      padding_value=-100\n",
        "#                  )\n",
        "\n",
        "#         return {\n",
        "#             \"pixel_values\":   pixel_values,\n",
        "#             \"input_ids\":      input_ids,\n",
        "#             \"attention_mask\": attention_mask,\n",
        "#             \"labels\":         labels,\n",
        "#         }\n",
        "\n",
        "\n",
        "# # === DEFINE A VALIDATION CALLBACK ===\n",
        "# class ValidationCallback(TrainerCallback):\n",
        "#     def __init__(self, processor, interval=500):\n",
        "#         self.processor = processor\n",
        "#         self.interval = interval\n",
        "\n",
        "#     def on_step_end(self, args, state, control, **kwargs):\n",
        "#         if state.global_step % self.interval == 0 and state.global_step > 0:\n",
        "#             model = kwargs.get('model', None)\n",
        "#             if model is None:\n",
        "#                 return\n",
        "\n",
        "#             model.eval()\n",
        "#             with torch.no_grad():\n",
        "#                 # Generate a prediction for a simple example\n",
        "#                 prompt = \"What color is the object in the image?\"\n",
        "#                 # Use a consistent dummy image size that reflects what the model expects after processing\n",
        "#                 dummy_image = Image.new(\"RGB\", (512, 512), (100, 150, 200))\n",
        "#                 inputs = self.processor(images=dummy_image,\n",
        "#                                   text=prompt, return_tensors=\"pt\")\n",
        "#                 inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "#                 # Generate output\n",
        "#                 generated_ids = model.generate(**inputs, max_length=20)\n",
        "#                 generated_text = self.processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "#                 print(f\"\\n=== VALIDATION AT STEP {state.global_step} ===\")\n",
        "#                 print(f\"Q: {prompt}\")\n",
        "#                 print(f\"A: {generated_text}\")\n",
        "#                 print(f\"Current training loss: {state.log_history[-1]['loss']:.4f}\")\n",
        "#                 print(f\"=== END VALIDATION ===\\n\")\n",
        "\n",
        "#             model.train()\n",
        "\n",
        "# # === TRAINER SETUP ===\n",
        "# validation_callback = ValidationCallback(processor)\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     data_collator= VQACollator(processor), # Use the custom collator instance\n",
        "#     callbacks=[validation_callback]\n",
        "# )\n",
        "\n",
        "# # === GPU INFO ===\n",
        "# if torch.cuda.is_available():\n",
        "#     print(\"GPU GPU Memory Usage Before Training:\")\n",
        "#     print(torch.cuda.memory_summary())\n",
        "\n",
        "# # === START TRAINING ===\n",
        "# trainer.train()\n",
        "\n",
        "# # === SAVE MODEL ===\n",
        "# trainer.save_model(\"./smol256_vqa_lora_r_16\")\n",
        "# print(\"Model saved to './smol256_vqa_lora_r_16'\")"
      ],
      "metadata": {
        "id": "ZGYYclYMcjE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# === Load processor and base model ===\n",
        "base_model = \"Salesforce/blip-vqa-base\"\n",
        "processor = BlipProcessor.from_pretrained(base_model)\n",
        "base = BlipForQuestionAnswering.from_pretrained(base_model)\n",
        "\n",
        "# === Load the LoRA-adapted model ===\n",
        "model = PeftModel.from_pretrained(base, \"./blip_vqa_lora_r_16\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === LOAD EVALUATION DATA ===\n",
        "# Ensure your eval_df has 'image_id', 'question', 'answer'\n",
        "# You can create it from raw data the same way you built train_df\n",
        "# Example:\n",
        "# eval_df = pd.read_csv(\"path/to/eval.csv\")\n",
        "\n",
        "eval_df['answer'] = eval_df['answer'].fillna('unknown').astype(str)\n",
        "eval_df['image_id'] = eval_df['image_id'].astype(str)\n",
        "\n",
        "\n",
        "# === INIT DATASET & DATALOADER ===\n",
        "eval_dataset = VQADataset(eval_df, processor, BASE_IMAGE_PATH, image_id_to_path)\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "image_ids = []\n",
        "\n",
        "for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Evaluating\"):\n",
        "    image_id = row[\"image_id\"]\n",
        "    question = row[\"question\"]\n",
        "    answer = row[\"answer\"]\n",
        "\n",
        "    # Locate image path (same logic as in your dataset)\n",
        "    if image_id in image_id_to_path:\n",
        "        relative_path = image_id_to_path[image_id]\n",
        "        image_path = os.path.join(BASE_IMAGE_PATH, relative_path)\n",
        "    else:\n",
        "        prefix = image_id[:2]\n",
        "        image_path = os.path.join(BASE_IMAGE_PATH, prefix, f\"{image_id}.jpg\")\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            # Try recursive fallback\n",
        "            for root, _, files in os.walk(BASE_IMAGE_PATH):\n",
        "                for file in files:\n",
        "                    if image_id in file:\n",
        "                        image_path = os.path.join(root, file)\n",
        "                        break\n",
        "\n",
        "    # Load image\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load image {image_id}: {e}\")\n",
        "        image = Image.new(\"RGB\", (224, 224), (0, 0, 0))  # fallback\n",
        "\n",
        "    # Preprocess and generate prediction\n",
        "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_length=20)\n",
        "        pred_answer = processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    predictions.append(pred_answer)\n",
        "    ground_truths.append(answer.strip())\n",
        "    image_ids.append(image_id)\n"
      ],
      "metadata": {
        "id": "TthsyNP68sbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "\n",
        "# Normalize text\n",
        "def normalize(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Exact match\n",
        "exact_match = [int(normalize(p) == normalize(g)) for p, g in zip(predictions, ground_truths)]\n",
        "accuracy = sum(exact_match) / len(exact_match)\n",
        "\n",
        "# Precision, Recall, F1 Score (macro-averaged over samples)\n",
        "# Convert to lowercase strings to ensure case-insensitive comparison\n",
        "y_true = [normalize(g) for g in ground_truths]\n",
        "y_pred = [normalize(p) for p in predictions]\n",
        "\n",
        "# Create the DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    \"image_id\": image_ids,\n",
        "    \"question\": eval_df[\"question\"],\n",
        "    \"ground_truth\": ground_truths,\n",
        "    \"prediction\": predictions,\n",
        "    \"exact_match\": exact_match\n",
        "})\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"\\nExact Match Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print and save results\n",
        "print(results_df.head())\n",
        "results_df.to_csv(\"vqa_eval_results.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "6pm-UMt1DtDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iwghIyYl9-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BWQv0x4YNlB0",
        "qzQyq7zp-opX",
        "M6ZM7Aa9ADDo",
        "2gJXzRKuX3E6",
        "Q6R1VOERCuHq"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}